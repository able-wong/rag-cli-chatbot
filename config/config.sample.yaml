# RAG CLI Chatbot Configuration
# Copy this file to config.yaml and customize for your setup

# ============================================================================
# LLM CONFIGURATION
# ============================================================================
llm:
  # PROVIDER SELECTION: Choose "ollama" or "gemini"
  provider: "ollama"  # Options: "ollama", "gemini"
  
  # OLLAMA SETTINGS (Local LLM server)
  # Use these settings when provider = "ollama"
  model: "llama3.2:3b"                  # Model name in Ollama
  base_url: "http://localhost:11434"    # Ollama server URL
  timeout: 120                          # Request timeout in seconds
  
  # GENERATION PARAMETERS
  temperature: 0.7                      # Randomness (0.0 = deterministic, 1.0 = creative)
  top_p: 0.9                           # Nucleus sampling
  top_k: 40                            # Top-k sampling
  max_tokens: 2048                     # Maximum response length
  
  # GEMINI SETTINGS (Google Cloud API)
  # Use these settings when provider = "gemini"
  # Uncomment and configure when switching to Gemini:
  gemini:
    api_key: "your-gemini-api-key-here"  # Your Gemini API key (or use GEMINI_API_KEY env var)
    model: "gemini-1.5-flash"            # Gemini model name
    
# ============================================================================
# EMBEDDING CONFIGURATION
# ============================================================================
embedding:
  # PROVIDER SELECTION: Choose "ollama", "gemini", or "sentence_transformers"
  provider: "sentence_transformers"  # Options: "ollama", "gemini", "sentence_transformers"
  
  # OLLAMA SETTINGS (Local embedding server)
  model: "nomic-embed-text"             # Recommended: 768 dimensions
  base_url: "http://localhost:11434"    # Ollama server URL
  timeout: 60                           # Request timeout in seconds
  
  # GEMINI SETTINGS (Google Cloud API)
  gemini:
    api_key: "your-gemini-api-key-here"  # Your Gemini API key (or use GEMINI_API_KEY env var)
    model: "text-embedding-004"          # Google's embedding model (768 dimensions)
  
  # SENTENCE TRANSFORMERS SETTINGS (Local Python library)
  sentence_transformers:
    model: "all-MiniLM-L6-v2"           # Lightweight model: 384 dimensions
    device: "cpu"                       # Options: "cpu", "cuda", "mps"

# ============================================================================
# VECTOR DATABASE CONFIGURATION
# ============================================================================
vector_db:
  provider: "qdrant"                    # Vector database type
  
  # LOCAL QDRANT SETTINGS
  host: "localhost"                     # Qdrant server host
  port: 6333                           # Qdrant server port
  
  # QDRANT CLOUD SETTINGS (Takes precedence if url is provided)
  # Uncomment for Qdrant Cloud:
  # url: "your-qdrant-cloud-url-here"    # Your Qdrant Cloud URL
  # api_key: "your-qdrant-api-key-here"  # Your Qdrant Cloud API key (or use QDRANT_API_KEY env var)
  
  # COLLECTION SETTINGS
  collection_name: "documents"          # Collection name for storing embeddings
  distance_metric: "cosine"             # Similarity metric

# ============================================================================
# RAG CONFIGURATION
# ============================================================================
rag:
  # TRIGGER SETTINGS
  trigger_phrase: "@knowledgebase"      # Phrase to trigger RAG search
  
  # RETRIEVAL STRATEGY SELECTION
  retrieval_strategy: "rewrite"         # Options: "rewrite", "hyde"
  
  # STRATEGY EXPLANATIONS:
  # 
  # "rewrite" (Default Strategy):
  #   - Transforms user queries into optimized search keywords
  #   - Fast and efficient for keyword-based matching
  #   - Works well for factual, direct questions
  #   - Lower computational cost
  #   - Example: "@knowledgebase How do neural networks work?" 
  #     → embedding_source_text: "neural networks training backpropagation"
  #
  # "hyde" (Hypothetical Document Embeddings):
  #   - Generates hypothetical documents that would answer the user's question
  #   - Better semantic matching through document-level understanding
  #   - Ideal for complex, conceptual questions requiring detailed explanations
  #   - Higher computational cost due to longer text generation
  #   - Example: "@knowledgebase How do neural networks work?"
  #     → embedding_source_text: "Neural networks learn through backpropagation..."
  #
  # PERFORMANCE TRADE-OFFS:
  # - "rewrite": Faster queries, good for keyword matching, may miss nuanced content
  # - "hyde": Slower queries, better semantic understanding, finds conceptually related content
  #
  # RECOMMENDATION:
  # - Start with "rewrite" for general use
  # - Switch to "hyde" if you need better retrieval of conceptually related documents
  # - Test both strategies with your specific knowledge base to determine optimal choice
  
  # HYBRID SEARCH SETTINGS
  use_hybrid_search: false             # Enable metadata filtering from natural language queries
  
  # HYBRID SEARCH EXPLANATION:
  # When enabled, the system can extract metadata filters from natural language queries
  # and apply them using three different filtering strategies:
  #
  # FILTER TYPES:
  # 1. Hard Filters (must match): "papers ONLY from 2025", "articles EXCLUSIVELY by Smith"
  # 2. Negation Filters (must not match): "papers not by Johnson", "excluding draft documents"
  # 3. Soft Filters (boost if match): "papers by Smith", "from 2024", "tagged Python"
  #
  # EXAMPLES:
  # - "papers by John Smith" → soft filter (boosts Smith's papers, still shows others)
  # - "articles ONLY from 2023" → hard filter (shows only 2023 articles)
  # - "documents not by Johnson" → negation filter (excludes Johnson's work)
  # - "Smith's papers about AI from March 2023" → combines soft filters for ranking
  #
  # Requirements:
  # - Qdrant collection must have payload indexes for: tags, author, publication_date
  # - These indexes are created during document ingestion
  # - System will log warnings if indexes are missing (performance impact)
  #
  # Benefits:
  # - More nuanced ranking with soft filters (see search_service configuration below)
  # - Precise filtering with hard/negation filters when needed
  # - Natural language interface - no need to learn complex filter syntax
  # - Works with both "rewrite" and "hyde" retrieval strategies
  
  # SEARCH SETTINGS
  top_k: 10                             # Number of documents to retrieve
  min_score: 0.3                       # Minimum similarity score threshold
  
  # CONTEXT SETTINGS
  max_context_length: 8000             # Maximum characters in RAG context
  
  # SYSTEM PROMPT
  system_prompt: |
    You are a helpful AI assistant with access to a knowledge base. 
    When provided with context from the knowledge base, use it to answer questions accurately.
    If the context doesn't contain relevant information, clearly state that you cannot answer based on the available knowledge base.
    Never make up information that isn't in the provided context.

# ============================================================================
# SPARSE EMBEDDING CONFIGURATION (Optional - enables exact phrase matching)
# ============================================================================
# Sparse embeddings enable exact phrase and keyword matching alongside semantic similarity.
# This is used for hybrid search strategies that combine dense and sparse vectors.
# Uncomment this section to enable sparse vector support:

sparse_embedding:
  # PROVIDER SELECTION: Currently only "splade" supported
  provider: "splade"  # Neural sparse retrieval provider
  
  # SPLADE SETTINGS (Neural sparse retrieval)
  splade:
    model: "naver/splade-cocondenser-ensembledistil"  # Pre-trained SPLADE model
    device: "cpu"                                     # Options: "cpu", "cuda", "mps"
    max_length: 512                                   # Maximum token length for input texts
    threshold: 0.01                                   # Threshold for filtering sparse vector values

# ============================================================================
# SEARCH SERVICE CONFIGURATION (Soft Filtering)
# ============================================================================
search_service:
  # SOFT FILTER BOOSTING CONFIGURATION
  # These settings control how soft filters boost document scores without excluding results
  
  # BASE BOOST SETTINGS
  base_boost_per_match: 0.12            # Base boost percentage for each filter match (12%)
  diminishing_factor: 0.75              # Diminishing returns factor (each subsequent match worth 75% of previous)
  max_total_boost: 0.6                  # Maximum total boost percentage (60% cap)
  
  # FIELD IMPORTANCE WEIGHTS
  # Higher weights mean more important fields get stronger boosts
  field_weights:
    tags: 1.2                           # Explicit tags are very important (120% weight)
    title: 1.0                          # Title matches are standard importance (100% weight)
    author: 0.8                         # Author matches are somewhat important (80% weight)
    publication_date: 0.6               # Date matches are less important (60% weight)
    file_extension: 0.4                 # File type is least important (40% weight)
    default: 0.7                        # Default weight for other fields (70% weight)
  
  # SEARCH BEHAVIOR SETTINGS
  fetch_multiplier: 4                   # Fetch 4x more documents when soft filtering (for re-ranking)
  
  # SOFT FILTER EXPLANATION:
  # Soft filters boost document scores based on metadata matches without excluding documents.
  # Unlike hard filters (must match) or negation filters (must not match), soft filters
  # increase relevance scores when metadata criteria are met.
  #
  # Example: "@knowledgebase papers by Smith about Python from 2024"
  # - Semantic search finds all relevant documents about Python
  # - Documents by "Smith" get author boost (0.12 * 0.8 = 9.6% boost)
  # - Documents from "2024" get date boost (0.12 * 0.6 = 7.2% boost)
  # - Documents with both get diminishing returns: 9.6% + (7.2% * 0.75) = 15% total boost
  # - Final ranking combines semantic similarity + metadata boosts
  #
  # Benefits:
  # - More nuanced ranking than hard filters
  # - Documents with preferred metadata rank higher
  # - Still shows relevant documents even if metadata doesn't match exactly
  # - Natural language queries work intuitively

# ============================================================================
# DOCUMENT CONFIGURATION
# ============================================================================
documents:
  # LOCAL DOCUMENT ROOT
  root_path: "/path/to/your/documents"  # Base path for local document files
  # Examples:
  # root_path: "/Users/username/Documents/knowledge_base"
  # root_path: "/home/user/documents"

# ============================================================================
# CLI CONFIGURATION
# ============================================================================
cli:
  # DISPLAY SETTINGS
  show_rag_results: true               # Show retrieved documents after RAG queries
  show_scores: true                    # Show similarity scores in results
  
  # CONVERSATION SETTINGS
  max_history_length: 20               # Maximum conversation turns to keep

# ============================================================================
# LOGGING CONFIGURATION
# ============================================================================
logging:
  # LOG LEVEL
  level: "INFO"                        # Log level: DEBUG, INFO, WARNING, ERROR, CRITICAL
  
  # OUTPUT DESTINATION
  output: "file"                       # Options: "console", "file", "stderr", "none"
  # - "console": Output to stdout (mixes with chat, good for development)
  # - "file": Output to log file (clean chat interface, good for production)
  # - "stderr": Output to stderr (separates from chat output)
  # - "none": Disable logging output (quiet mode)
  
  # FILE LOGGING SETTINGS (used when output = "file")
  file:
    path: "logs/rag-cli.log"           # Log file path (relative to project root)
    max_size: "10MB"                   # Max file size before rotation
    backup_count: 5                    # Number of backup files to keep
    
  # LOG FORMAT
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"  # Log message format
  date_format: "%Y-%m-%d %H:%M:%S"     # Timestamp format
