# RAG CLI Chatbot

A command-line chatbot with Retrieval-Augmented Generation (RAG) capabilities. Supports knowledge base search using Qdrant vector database and various LLM providers.

## ğŸ¯ Features

- **Interactive CLI Interface**: Rich terminal interface with colors and formatting
- **RAG Capabilities**: Search and retrieve information from knowledge base
- **Multiple LLM Providers**: Support for Ollama and Gemini
- **Multiple Embedding Providers**: Ollama, Gemini, and SentenceTransformers
- **Vector Database**: Qdrant integration for semantic search
- **Smart Fallback**: "No answer" response when confidence is low
- **Conversation Management**: Maintains chat history with configurable limits
- **Command System**: Built-in commands for enhanced user experience

## ğŸš€ Quick Start

### Prerequisites

- Python 3.11+
- Virtual environment (recommended)

### Installation

1. **Clone the repository** (or use the existing project):
   ```bash
   cd rag-cli-chatbot
   ```

2. **Create virtual environment**:
   ```bash
   python3 -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```

3. **Install dependencies**:
   ```bash
   pip install -r requirements.txt
   ```

4. **Configure the application**:
   ```bash
   # Copy sample configuration and customize
   cp config/config.sample.yaml config/config.yaml
   # Edit config/config.yaml with your settings
   # See configuration section below
   ```

5. **Run the chatbot**:
   ```bash
   python3 main.py
   ```

## âš™ï¸ Configuration

The application uses `config/config.yaml` for configuration. Key settings:

### LLM Provider Setup

**Ollama (Local)**:
```yaml
llm:
  provider: "ollama"
  model: "llama3.2"
  base_url: "http://localhost:11434"
```

**Gemini (Cloud)**:
```yaml
llm:
  provider: "gemini"
  gemini:
    api_key: "your-api-key"  # Or set GEMINI_API_KEY env var
    model: "gemini-1.5-flash"
```

### Embedding Provider Setup

**SentenceTransformers (Local, Default)**:
```yaml
embedding:
  provider: "sentence_transformers"
  sentence_transformers:
    model: "all-MiniLM-L6-v2"
    device: "cpu"
```

**Ollama (Local)**:
```yaml
embedding:
  provider: "ollama"
  model: "nomic-embed-text"
```

### Vector Database Setup

**Local Qdrant**:
```yaml
vector_db:
  provider: "qdrant"
  host: "localhost"
  port: 6333
  collection_name: "knowledge_base"
```

**Qdrant Cloud**:
```yaml
vector_db:
  provider: "qdrant"
  url: "https://your-cluster.qdrant.io:6333"
  api_key: "your-api-key"  # Or set QDRANT_API_KEY env var
```

### Qdrant collection "documents" Schema Keys:

- **chunk_text**: The actual text content of the chunk
- **original_text**: Original unprocessed document text
- **chunk_index**: Sequential index of this chunk within the document
- **source_url**: Source URL with protocol (e.g., file:documents/article.pdf, https://example.com/doc.html)
- **file_extension**: File extension (e.g., .pdf, .html, .txt)
- **file_size**: File size in bytes
- **last_modified**: Last modification timestamp (ISO format)
- **content_hash**: SHA256 hash of document content
- **author**: Author name extracted from content or source URL path (string or null)
- **title**: Document title from content or cleaned filename (string)
- **publication_date**: Publication date in YYYY-MM-DD format (ISO string or null)
- **tags**: Array of relevant keywords/topics from content (array of strings)

## ğŸ® Usage

### Basic Commands

- **General Chat**: Type normally to chat without knowledge base
- **Knowledge Base Search**: Use `@knowledgebase` to trigger RAG search
- **Clear History**: `/clear` - Reset conversation
- **Exit**: `/bye` - Exit the application
- **View Documents**: `/doc <number>` - See full document content

### Example Session

```bash
$ python3 main.py

ğŸ¤– Welcome to RAG CLI Chatbot!

You: Hello, how are you?
Assistant: Hello! I'm doing well, thank you for asking...

You: @knowledgebase What is Python?
ğŸ” Searching knowledge base...
Assistant: Based on the knowledge base, Python is a high-level programming language...

ğŸ“š Retrieved from knowledge base:
â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ # â”ƒ Source                                      â”ƒ Score  â”ƒ
â”¡â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 1 â”‚ python_docs.md                              â”‚ 0.892  â”‚
â”‚ 2 â”‚ programming_guide.txt                       â”‚ 0.845  â”‚
â””â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
ğŸ’¡ Use /doc <number> to see full content
```

## ğŸ§ª Testing

To run the full test suite, use `pytest`:

```bash
# Activate virtual environment
source venv/bin/activate

# Run all tests
pytest
```

This will discover and run all tests in the `tests/` directory. For more detailed output, you can use `pytest -v`.

## ğŸ“ Project Structure

```
rag-cli-chatbot/
â”œâ”€â”€ src/                    # Source code
â”‚   â”œâ”€â”€ cli.py             # Main CLI interface
â”‚   â”œâ”€â”€ config_manager.py  # Configuration management
â”‚   â”œâ”€â”€ embedding_client.py # Embedding generation
â”‚   â”œâ”€â”€ llm_client.py      # LLM interaction
â”‚   â”œâ”€â”€ qdrant_db.py       # Vector database client
â”‚   â””â”€â”€ logging_config.py  # Logging setup
â”œâ”€â”€ config/                # Configuration files
â”‚   â”œâ”€â”€ config.sample.yaml # Sample configuration template
â”‚   â””â”€â”€ config.yaml        # Main configuration (user-created)
â”œâ”€â”€ tests/                 # Test files
â”‚   â”œâ”€â”€ test_config_manager.py
â”‚   â”œâ”€â”€ test_phase1_clients.py
â”‚   â””â”€â”€ test_cli_moked.py
â”œâ”€â”€ main.py               # Application entry point
â”œâ”€â”€ requirements.txt      # Python dependencies
â””â”€â”€ README.md            # This file
```

## ğŸ”§ Development

### Phase Implementation Status

- âœ… **Phase 0**: Project setup, configuration, logging
- âœ… **Phase 1**: Core service integration (LLM, Embeddings, Qdrant)
- âœ… **Phase 2**: MVP CLI with RAG and fallback logic

### Key Features Implemented

- [x] Configuration management with environment variable support
- [x] Multiple LLM providers (Ollama, Gemini)
- [x] Multiple embedding providers (SentenceTransformers, Ollama, Gemini) 
- [x] Qdrant vector database integration
- [x] RAG trigger detection (@knowledgebase)
- [x] Confidence-based fallback system
- [x] Rich CLI interface with formatted output
- [x] Conversation history management
- [x] Document detail viewing
- [x] Comprehensive test suite

## ğŸ”® Future Enhancements

- **Document Ingestion Pipeline**: Automated document processing and chunking
- **Query Rewriting**: LLM-based query enhancement
- **Hybrid Search**: Combine vector and keyword search
- **Re-ranking**: Advanced result scoring and ordering
- **Implicit RAG**: Automatic knowledge base routing
- **Web Interface**: Browser-based chat interface

## ğŸ› Troubleshooting

### Common Issues

1. **"Module not found" errors**: Ensure virtual environment is activated
2. **Qdrant connection failed**: Check if Qdrant server is running
3. **LLM timeout**: Adjust timeout settings in config
4. **Empty knowledge base**: Ensure documents are properly ingested

### Debug Mode

Enable debug logging in `config.yaml`:
```yaml
logging:
  level: "DEBUG"
```

## ğŸ“„ License

This project is open source and available under the [MIT License](LICENSE).

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests for new functionality
5. Submit a pull request

## ğŸ“ Support

For issues and questions:
- Create an issue on GitHub
- Check the troubleshooting section
- Review the configuration examples
